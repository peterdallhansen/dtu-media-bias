{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.etree import iterparse\n",
    "import xml\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434ac67",
   "metadata": {},
   "source": [
    "## Load & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92759f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(path, dim):\n",
    "    '''\n",
    "    read the glove vectors from path with dimension dim\n",
    "    '''\n",
    "    df = pd.read_csv(path + 'glove.6B.' + str(dim) + 'd.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    glove = {key: val.values for key, val in df.T.items()}\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea8e00",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "def cleanQuotations(text):\n",
    "    text = re.sub(r'[`‘’‛⸂⸃⸌⸍⸜⸝]', \"'\", text)\n",
    "    text = re.sub(r'[„“”]|(\\'\\')|(,,)', '\"', text)\n",
    "    return text\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'(www\\S+)|(https?\\S+)|(href)', ' ', text)\n",
    "    text = re.sub(r'\\{[^}]*\\}|\\[[^]]*\\]|\\([^)]*\\)', ' ', text)\n",
    "    text = re.sub(r'Getty [Ii]mages?|Getty|[Ff]ollow us on [Tt]witter|MORE:|ADVERTISEMENT|VIDEO', ' ', text)\n",
    "    text = re.sub(r'@\\S+|#\\S+|\\.{2,}', ' ', text)\n",
    "    text = text.lstrip().replace('\\n','')\n",
    "    text = re.sub(r'  +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def fixup(text):\n",
    "    text = text.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\") \\\n",
    "               .replace('nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\") \\\n",
    "               .replace('quot;', \"'\").replace('<br />', \"\\n\").replace('\\\\\"', '\"') \\\n",
    "               .replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(' @-@ ', '-') \\\n",
    "               .replace('\\\\', ' \\\\ ')\n",
    "    return html.unescape(text)\n",
    "\n",
    "def textCleaning(title, text):\n",
    "    title = cleanQuotations(title)\n",
    "    text  = cleanQuotations(text)\n",
    "    text  = cleanText(fixup(text))\n",
    "    return (title + \". \" + text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615dec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from lxml import etree as ET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "def parse_articles(article_path):\n",
    "    for _, elem in ET.iterparse(article_path, events=(\"end\",)):\n",
    "        if elem.tag != \"article\":\n",
    "            continue\n",
    "\n",
    "        article_id = elem.get(\"id\")\n",
    "        title = elem.get(\"title\", \"\") or \"\"\n",
    "\n",
    "        paragraphs = [\n",
    "            (p.text or \"\").strip()\n",
    "            for p in elem.findall(\"p\")\n",
    "            if p.text\n",
    "        ]\n",
    "\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "\n",
    "        yield article_id, title, text\n",
    "        elem.clear()  \n",
    "\n",
    "def parse_labels(label_path):\n",
    "    labels = {}\n",
    "\n",
    "    for _, elem in ET.iterparse(label_path, events=(\"end\",)):\n",
    "        if elem.tag != \"article\":\n",
    "            continue\n",
    "\n",
    "        labels[elem.get(\"id\")] = {\n",
    "            \"hyperpartisan\": elem.get(\"hyperpartisan\"),\n",
    "            \"bias\": elem.get(\"bias\"),\n",
    "        }\n",
    "\n",
    "        elem.clear()\n",
    "\n",
    "    return labels\n",
    "\n",
    "def preprocess_article(title, text):\n",
    "    # your pipeline + final normalization layer\n",
    "    cleaned = textCleaning(title, text)\n",
    "    cleaned = cleaned.lower()\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(article_path, label_path, preprocess=None):\n",
    "    labels = parse_labels(label_path)\n",
    "\n",
    "    for article_id, title, text in parse_articles(article_path):\n",
    "        if article_id not in labels:\n",
    "            continue\n",
    "\n",
    "        if preprocess:\n",
    "            text = preprocess_article(title, text)\n",
    "\n",
    "        yield {\n",
    "            \"id\": article_id,\n",
    "            \"title\": title,\n",
    "            \"text\": text,\n",
    "            \"bias\": labels[article_id][\"bias\"],\n",
    "            \"hyperpartisan\": labels[article_id][\"hyperpartisan\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833128df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleDataset(IterableDataset):\n",
    "    def __init__(self, article_path, label_path):\n",
    "        self.article_path = article_path\n",
    "        self.label_path = label_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from load_dataset(\n",
    "            self.article_path,\n",
    "            self.label_path,\n",
    "            preprocess=preprocess_article\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5646174",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e05a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdh/Documents/GitHub/DTU/Intelligente Systemer/Exam/Project/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "def hf_tokenize(text):\n",
    "    return tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "\n",
    "def build_vocab_from_stream(dataset_stream, min_freq=3, max_size=None):\n",
    "    counter = Counter()\n",
    "\n",
    "    for sample in tqdm(dataset_stream, desc=\"Building vocab\", total=600000):\n",
    "        tokens = [w for w, _ in hf_tokenize(sample[\"text\"])]\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    idx = 2\n",
    "\n",
    "    for token, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        if max_size and idx >= max_size:\n",
    "            break\n",
    "\n",
    "        vocab[token] = idx\n",
    "        idx += 1\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6517c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "dataPath = \"./Dataset\"\n",
    "\n",
    "train_article_path = os.path.join(dataPath, \"train-articles.xml\")\n",
    "train_label_path   = os.path.join(dataPath, \"ground-truth-training-bypublisher-20181122.xml\")\n",
    "\n",
    "val_article_path = os.path.join(dataPath, \"val-articles.xml\")\n",
    "val_label_path   = os.path.join(dataPath, \"ground-truth-validation-bypublisher-20181122.xml\")\n",
    "\n",
    "\n",
    "train_dataset = ArticleDataset(train_article_path, train_label_path)\n",
    "val_dataset   = ArticleDataset(val_article_path, val_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72336195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab:  80%|████████  | 600000/750000 [08:00<02:00, 1249.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Building vocab...\")\n",
    "vocab = build_vocab_from_stream(\n",
    "    load_dataset(train_article_path, train_label_path, preprocess_article),\n",
    "    min_freq=3,\n",
    "    max_size=50000\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4d6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(tokens, vocab):\n",
    "    unk = vocab[\"<unk>\"]\n",
    "    return [vocab.get(t, unk) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb4c023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_batch(batch, vocab, max_len=256):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for sample in batch:\n",
    "        tokens = word_tokenize(sample[\"text\"])\n",
    "        ids = numericalize(tokens[:max_len], vocab)\n",
    "\n",
    "        pad = max_len - len(ids)\n",
    "        if pad > 0:\n",
    "            ids += [vocab[\"<pad>\"]] * pad\n",
    "\n",
    "        texts.append(ids)\n",
    "\n",
    "        # choose target — here: hyperpartisan binary\n",
    "        label = 1 if sample[\"hyperpartisan\"] == \"true\" else 0\n",
    "        labels.append(label)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(texts, dtype=torch.long),\n",
    "        torch.tensor(labels, dtype=torch.long)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ae390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_batch(batch, vocab, max_len=256):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for sample in batch:\n",
    "        tokens = word_tokenize(sample[\"text\"])\n",
    "        ids = numericalize(tokens[:max_len], vocab)\n",
    "\n",
    "        pad = max_len - len(ids)\n",
    "        if pad > 0:\n",
    "            ids += [vocab[\"<pad>\"]] * pad\n",
    "\n",
    "        texts.append(ids)\n",
    "\n",
    "        # choose target — here: hyperpartisan binary\n",
    "        label = 1 if sample[\"hyperpartisan\"] == \"true\" else 0\n",
    "        labels.append(label)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(texts, dtype=torch.long),\n",
    "        torch.tensor(labels, dtype=torch.long)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0808d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=partial(collate_batch, vocab=vocab, max_len=256)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=partial(collate_batch, vocab=vocab, max_len=256)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "addaddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes=2,\n",
    "                 kernel_sizes=(3,4,5), num_filters=100,\n",
    "                 padding_idx=0, pretrained_embeddings=None,\n",
    "                 freeze_embeddings=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        # optionally load pretrained GloVe\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "        # optionally freeze embeddings\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # conv layers with different kernel sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "\n",
    "        x = self.embedding(x)           # (batch, seq, embed)\n",
    "        x = x.transpose(1, 2)           # (batch, embed, seq)\n",
    "\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            c = conv(x)                 # (batch, num_filters, L_out)\n",
    "            c = F.relu(c)\n",
    "            c = F.max_pool1d(c, c.size(2))   # (batch, num_filters, 1)\n",
    "            conv_outputs.append(c.squeeze(2))\n",
    "\n",
    "        out = torch.cat(conv_outputs, dim=1)  # (batch, F*kernels)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ca54a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "num_classes = 2  # partisan / not partisan\n",
    "\n",
    "model = TextCNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=embed_dim,\n",
    "    num_classes=num_classes,\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    pretrained_embeddings=None,   # or pass GloVe tensor later\n",
    "    freeze_embeddings=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2dfe848",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba9531bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for tokens, labels in loader:\n",
    "        tokens = tokens.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(tokens)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16cf6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for tokens, labels in loader:\n",
    "        tokens = tokens.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(tokens)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_one_epoch(train_loader)\n",
    "    val_loss, val_acc = eval_epoch(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"  Train loss: {train_loss:.4f}  acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   loss: {val_loss:.4f}  acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557452c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
