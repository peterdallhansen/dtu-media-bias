{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration - SemEval-2019 Task 4\n",
    "Exploring the hyperpartisan news detection dataset before and after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import Counter\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raw Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_articles(xml_path, limit=5):\n",
    "    articles = []\n",
    "    context = etree.iterparse(xml_path, events=('end',), tag='article')\n",
    "    for i, (event, elem) in enumerate(context):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        articles.append({\n",
    "            'id': elem.get('id'),\n",
    "            'title': elem.get('title', ''),\n",
    "            'published': elem.get('published-at', ''),\n",
    "            'raw_xml': etree.tostring(elem, encoding='unicode', pretty_print=True)[:2000]\n",
    "        })\n",
    "        elem.clear()\n",
    "    return articles\n",
    "\n",
    "def parse_raw_labels(xml_path):\n",
    "    labels = {}\n",
    "    context = etree.iterparse(xml_path, events=('end',), tag='article')\n",
    "    for event, elem in context:\n",
    "        labels[elem.get('id')] = {\n",
    "            'hyperpartisan': elem.get('hyperpartisan'),\n",
    "            'labeled_by': elem.get('labeled-by'),\n",
    "            'url': elem.get('url', '')\n",
    "        }\n",
    "        elem.clear()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading raw training data...\")\n",
    "raw_articles = parse_raw_articles(config.ARTICLES_TRAIN, limit=3)\n",
    "raw_labels = parse_raw_labels(config.LABELS_TRAIN)\n",
    "print(f\"Total labels: {len(raw_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE RAW ARTICLES\")\n",
    "print(\"=\" * 80)\n",
    "for art in raw_articles:\n",
    "    label_info = raw_labels.get(art['id'], {})\n",
    "    print(f\"\\nID: {art['id']}\")\n",
    "    print(f\"Title: {art['title'][:100]}...\" if len(art['title']) > 100 else f\"Title: {art['title']}\")\n",
    "    print(f\"Published: {art['published']}\")\n",
    "    print(f\"Hyperpartisan: {label_info.get('hyperpartisan')}\")\n",
    "    print(f\"\\nRaw XML (truncated):\")\n",
    "    print(art['raw_xml'][:1000])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = Counter(v['hyperpartisan'] for v in raw_labels.values())\n",
    "print(\"Training set label distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(raw_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = parse_raw_labels(config.LABELS_TEST)\n",
    "test_counts = Counter(v['hyperpartisan'] for v in test_labels.values())\n",
    "print(\"Test set label distribution:\")\n",
    "for label, count in test_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(test_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess_and_cache, load_cached_data\n",
    "\n",
    "if not (config.CACHE_DIR / \"train_data.pkl\").exists():\n",
    "    print(\"Cache not found, running preprocessing...\")\n",
    "    preprocess_and_cache()\n",
    "else:\n",
    "    print(\"Cache exists, loading...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_cached_data('train')\n",
    "test_data = load_cached_data('test')\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessed Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE PREPROCESSED ARTICLES\")\n",
    "print(\"=\" * 80)\n",
    "for sample in train_data[:3]:\n",
    "    print(f\"\\nID: {sample['id']}\")\n",
    "    print(f\"Title: {sample['title'][:100]}...\" if len(sample['title']) > 100 else f\"Title: {sample['title']}\")\n",
    "    print(f\"Label: {sample['label']} ({'hyperpartisan' if sample['label'] else 'not hyperpartisan'})\")\n",
    "    print(f\"Num tokens: {len(sample['tokens'])}\")\n",
    "    print(f\"Num hyperlinks: {len(sample['hyperlinks'])}\")\n",
    "    print(f\"\\nFirst 50 tokens: {sample['tokens'][:50]}\")\n",
    "    print(f\"\\nCleaned text (first 500 chars): {sample['text'][:500]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lengths = [len(d['tokens']) for d in train_data]\n",
    "print(\"Token length statistics (train):\")\n",
    "print(f\"  Min: {min(token_lengths)}\")\n",
    "print(f\"  Max: {max(token_lengths)}\")\n",
    "print(f\"  Mean: {sum(token_lengths)/len(token_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(token_lengths)[len(token_lengths)//2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for d in train_data:\n",
    "    all_tokens.extend(d['tokens'])\n",
    "word_freq = Counter(all_tokens)\n",
    "print(f\"\\nTotal tokens: {len(all_tokens)}\")\n",
    "print(f\"Unique tokens: {len(word_freq)}\")\n",
    "print(f\"\\nTop 20 most common words:\")\n",
    "for word, count in word_freq.most_common(20):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperlink Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_counts = [len(d['hyperlinks']) for d in train_data]\n",
    "print(\"Hyperlink statistics (train):\")\n",
    "print(f\"  Articles with links: {sum(1 for c in hyperlink_counts if c > 0)}\")\n",
    "print(f\"  Total links: {sum(hyperlink_counts)}\")\n",
    "print(f\"  Mean per article: {sum(hyperlink_counts)/len(hyperlink_counts):.1f}\")\n",
    "print(f\"  Max: {max(hyperlink_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_types = Counter()\n",
    "for d in train_data:\n",
    "    for link in d['hyperlinks']:\n",
    "        link_types[link['type']] += 1\n",
    "print(f\"\\nLink types: {dict(link_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Hyperpartisan vs Non-Hyperpartisan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = [d for d in train_data if d['label'] == 1]\n",
    "non_hyper = [d for d in train_data if d['label'] == 0]\n",
    "\n",
    "print(f\"Hyperpartisan articles: {len(hyper)}\")\n",
    "print(f\"Non-hyperpartisan articles: {len(non_hyper)}\")\n",
    "\n",
    "hyper_len = [len(d['tokens']) for d in hyper]\n",
    "non_hyper_len = [len(d['tokens']) for d in non_hyper]\n",
    "\n",
    "print(f\"\\nAvg tokens (hyperpartisan): {sum(hyper_len)/len(hyper_len):.1f}\")\n",
    "print(f\"Avg tokens (non-hyperpartisan): {sum(non_hyper_len)/len(non_hyper_len):.1f}\")\n",
    "\n",
    "hyper_links = [len(d['hyperlinks']) for d in hyper]\n",
    "non_hyper_links = [len(d['hyperlinks']) for d in non_hyper]\n",
    "\n",
    "print(f\"\\nAvg hyperlinks (hyperpartisan): {sum(hyper_links)/len(hyper_links):.1f}\")\n",
    "print(f\"Avg hyperlinks (non-hyperpartisan): {sum(non_hyper_links)/len(non_hyper_links):.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
