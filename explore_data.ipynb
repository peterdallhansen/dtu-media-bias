{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration - SemEval-2019 Task 4\n",
    "Exploring the hyperpartisan news detection dataset before and after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import Counter\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raw Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_articles(xml_path, limit=5):\n",
    "    articles = []\n",
    "    context = etree.iterparse(xml_path, events=('end',), tag='article')\n",
    "    for i, (event, elem) in enumerate(context):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        articles.append({\n",
    "            'id': elem.get('id'),\n",
    "            'title': elem.get('title', ''),\n",
    "            'published': elem.get('published-at', ''),\n",
    "            'raw_xml': etree.tostring(elem, encoding='unicode', pretty_print=True)[:2000]\n",
    "        })\n",
    "        elem.clear()\n",
    "    return articles\n",
    "\n",
    "def parse_raw_labels(xml_path):\n",
    "    labels = {}\n",
    "    context = etree.iterparse(xml_path, events=('end',), tag='article')\n",
    "    for event, elem in context:\n",
    "        labels[elem.get('id')] = {\n",
    "            'hyperpartisan': elem.get('hyperpartisan'),\n",
    "            'labeled_by': elem.get('labeled-by'),\n",
    "            'url': elem.get('url', '')\n",
    "        }\n",
    "        elem.clear()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw training data...\n",
      "Total labels: 645\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading raw training data...\")\n",
    "raw_articles = parse_raw_articles(config.ARTICLES_TRAIN, limit=3)\n",
    "raw_labels = parse_raw_labels(config.LABELS_TRAIN)\n",
    "print(f\"Total labels: {len(raw_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE RAW ARTICLES\n",
      "================================================================================\n",
      "\n",
      "ID: 0000000\n",
      "Title: Kucinich: Reclaiming the money power\n",
      "Published: 2017-09-10\n",
      "Hyperpartisan: true\n",
      "\n",
      "Raw XML (truncated):\n",
      "<article id=\"0000000\" published-at=\"2017-09-10\" title=\"Kucinich: Reclaiming the money power\">\n",
      "From flickr.com: Money {MID-161793} <p>Money ( <a href=\"https://farm8.static.flickr.com/7020/6551534889_9c8ae52997.jpg\" type=\"external\">Image</a> by <a href=\"https://www.flickr.com/people/68751915@N05/\" type=\"external\">401(K) 2013</a>) <a href=\"https://creativecommons.org/licenses/by-sa/2.0/\" type=\"external\">Permission</a> <a type=\"internal\">Details</a> <a type=\"internal\">DMCA</a></p> No Pill Can Stop Tinnitus, But This 1 Weird Trick Can <p>The walls are closing in on Congress.</p> <p>Terrifying walls of water from Hurricanes Harvey and Irma, which, when the damage is totaled, could rise to a half trillion dollars. The Walls of War: The multi-trillion dollar ongoing cost of Afghanistan, Iraq and other interventions. The crumbling walls of the U.S. infrastructure, which need at least $3 trillion to be repaired or replaced. A wall of 11 million undocumented immigrants, whose deportation could ea\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 0000001\n",
      "Title: Trump Just Woke Up & Viciously Attacked Puerto Ricans On Twitter Like A Cruel Old Man\n",
      "Published: 2017-10-12\n",
      "Hyperpartisan: true\n",
      "\n",
      "Raw XML (truncated):\n",
      "<article id=\"0000001\" published-at=\"2017-10-12\" title=\"Trump Just Woke Up &amp; Viciously Attacked Puerto Ricans On Twitter Like A Cruel Old Man\"><p>Donald Trump ran on many braggadocios and largely unrealistic campaign promises. One of <a href=\"http://www.cnn.com/2017/03/16/politics/trump-infrastructure/index.html\" type=\"external\">those promises</a> was to be the best, the hugest, the most competent infrastructure president the United States has ever seen. Trump was going to fix every infrastructure problem in the country and Make America Great Again in the process.</p> <p>That is, unless you’re a brown American. In that case, you’re on your own, even after a massive natural disaster like Hurricane Maria.</p> <p>Puerto Rico’s debt, which the Puerto Rican citizens not in government would have no responsibility for, has nothing to do with using federal emergency disaster funds to save the lives of American citizens there. The infrastructure is certainly a mess at this point after a Cate\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 0000002\n",
      "Title: Liberals wailing about gun control, but what about abortion?\n",
      "Published: 2017-10-11\n",
      "Hyperpartisan: true\n",
      "\n",
      "Raw XML (truncated):\n",
      "<article id=\"0000002\" published-at=\"2017-10-11\" title=\"Liberals wailing about gun control, but what about abortion?\">\n",
      "Photo By Justin Sullivan/Getty Images <p>In response to Joyce Newman’s recent letter about a conversation about guns: According to the National Right to Life Organization, approximately 600,000 babies are murdered every year by Planned Parenthood with more than 52 million murdered since Roe v. Wade. This makes Planned Parenthood the biggest mass murderer in the history of the world. Is she willing to have a serious conversation about that? Where is her outrage over that?</p> <p>More people die every year from overdoses or auto accidents then from guns. More people die every year from obesity then from guns. Where is her outrage over those issues?</p> <p>The left’s obsession with gun “control” is just that, control. It has always been about Democrats wanting to control every aspect of your life. They support Planned Parenthood but go ballistic when a gun is used to kill \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE RAW ARTICLES\")\n",
    "print(\"=\" * 80)\n",
    "for art in raw_articles:\n",
    "    label_info = raw_labels.get(art['id'], {})\n",
    "    print(f\"\\nID: {art['id']}\")\n",
    "    print(f\"Title: {art['title'][:100]}...\" if len(art['title']) > 100 else f\"Title: {art['title']}\")\n",
    "    print(f\"Published: {art['published']}\")\n",
    "    print(f\"Hyperpartisan: {label_info.get('hyperpartisan')}\")\n",
    "    print(f\"\\nRaw XML (truncated):\")\n",
    "    print(art['raw_xml'][:1000])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      "  true: 238 (36.9%)\n",
      "  false: 407 (63.1%)\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(v['hyperpartisan'] for v in raw_labels.values())\n",
    "print(\"Training set label distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(raw_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set label distribution:\n",
      "  false: 314 (50.0%)\n",
      "  true: 314 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "test_labels = parse_raw_labels(config.LABELS_TEST)\n",
    "test_counts = Counter(v['hyperpartisan'] for v in test_labels.values())\n",
    "print(\"Test set label distribution:\")\n",
    "for label, count in test_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(test_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache exists, loading...\n"
     ]
    }
   ],
   "source": [
    "from preprocess import preprocess_and_cache, load_cached_data\n",
    "\n",
    "if not (config.CACHE_DIR / \"train_data.pkl\").exists():\n",
    "    print(\"Cache not found, running preprocessing...\")\n",
    "    preprocess_and_cache()\n",
    "else:\n",
    "    print(\"Cache exists, loading...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 645\n",
      "Test samples: 628\n"
     ]
    }
   ],
   "source": [
    "train_data = load_cached_data('train')\n",
    "test_data = load_cached_data('test')\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessed Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE PREPROCESSED ARTICLES\n",
      "================================================================================\n",
      "\n",
      "ID: 0000000\n",
      "Title: Kucinich: Reclaiming the money power\n",
      "Label: 1 (hyperpartisan)\n",
      "Num tokens: 1465\n",
      "Num hyperlinks: 6\n",
      "\n",
      "First 50 tokens: ['kucinich', 'reclaiming', 'the', 'money', 'power', 'from', 'flickr', 'com', 'money', 'mid', 'money', 'image', 'by', 'k', 'permission', 'details', 'dmca', 'no', 'pill', 'can', 'stop', 'tinnitus', 'but', 'this', 'weird', 'trick', 'can', 'the', 'walls', 'are', 'closing', 'in', 'on', 'congress', 'terrifying', 'walls', 'of', 'water', 'from', 'hurricanes', 'harvey', 'and', 'irma', 'which', 'when', 'the', 'damage', 'is', 'totaled', 'could']\n",
      "\n",
      "Cleaned text (first 500 chars): kucinich reclaiming the money power from flickr com money mid money image by k permission details dmca no pill can stop tinnitus but this weird trick can the walls are closing in on congress terrifying walls of water from hurricanes harvey and irma which when the damage is totaled could rise to a half trillion dollars the walls of war the multi trillion dollar ongoing cost of afghanistan iraq and other interventions the crumbling walls of the u s infrastructure which need at least trillion to be...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 0000001\n",
      "Title: Trump Just Woke Up & Viciously Attacked Puerto Ricans On Twitter Like A Cruel Old Man\n",
      "Label: 1 (hyperpartisan)\n",
      "Num tokens: 254\n",
      "Num hyperlinks: 3\n",
      "\n",
      "First 50 tokens: ['trump', 'just', 'woke', 'up', 'viciously', 'attacked', 'puerto', 'ricans', 'on', 'twitter', 'like', 'a', 'cruel', 'old', 'man', 'donald', 'trump', 'ran', 'on', 'many', 'braggadocios', 'and', 'largely', 'unrealistic', 'campaign', 'promises', 'one', 'of', 'those', 'promises', 'was', 'to', 'be', 'the', 'best', 'the', 'hugest', 'the', 'most', 'competent', 'infrastructure', 'president', 'the', 'united', 'states', 'has', 'ever', 'seen', 'trump', 'was']\n",
      "\n",
      "Cleaned text (first 500 chars): trump just woke up viciously attacked puerto ricans on twitter like a cruel old man donald trump ran on many braggadocios and largely unrealistic campaign promises one of those promises was to be the best the hugest the most competent infrastructure president the united states has ever seen trump was going to fix every infrastructure problem in the country and make america great again in the process that is unless you re a brown american in that case you re on your own even after a massive natur...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 0000002\n",
      "Title: Liberals wailing about gun control, but what about abortion?\n",
      "Label: 1 (hyperpartisan)\n",
      "Num tokens: 178\n",
      "Num hyperlinks: 0\n",
      "\n",
      "First 50 tokens: ['liberals', 'wailing', 'about', 'gun', 'control', 'but', 'what', 'about', 'abortion', 'photo', 'by', 'justin', 'sullivan', 'getty', 'images', 'in', 'response', 'to', 'joyce', 'newman', 's', 'recent', 'letter', 'about', 'a', 'conversation', 'about', 'guns', 'according', 'to', 'the', 'national', 'right', 'to', 'life', 'organization', 'approximately', 'babies', 'are', 'murdered', 'every', 'year', 'by', 'planned', 'parenthood', 'with', 'more', 'than', 'million', 'murdered']\n",
      "\n",
      "Cleaned text (first 500 chars): liberals wailing about gun control but what about abortion photo by justin sullivan getty images in response to joyce newman s recent letter about a conversation about guns according to the national right to life organization approximately babies are murdered every year by planned parenthood with more than million murdered since roe v wade this makes planned parenthood the biggest mass murderer in the history of the world is she willing to have a serious conversation about that where is her outr...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE PREPROCESSED ARTICLES\")\n",
    "print(\"=\" * 80)\n",
    "for sample in train_data[:3]:\n",
    "    print(f\"\\nID: {sample['id']}\")\n",
    "    print(f\"Title: {sample['title'][:100]}...\" if len(sample['title']) > 100 else f\"Title: {sample['title']}\")\n",
    "    print(f\"Label: {sample['label']} ({'hyperpartisan' if sample['label'] else 'not hyperpartisan'})\")\n",
    "    print(f\"Num tokens: {len(sample['tokens'])}\")\n",
    "    print(f\"Num hyperlinks: {len(sample['hyperlinks'])}\")\n",
    "    print(f\"\\nFirst 50 tokens: {sample['tokens'][:50]}\")\n",
    "    print(f\"\\nCleaned text (first 500 chars): {sample['text'][:500]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length statistics (train):\n",
      "  Min: 24\n",
      "  Max: 5649\n",
      "  Mean: 583.6\n",
      "  Median: 425\n"
     ]
    }
   ],
   "source": [
    "token_lengths = [len(d['tokens']) for d in train_data]\n",
    "print(\"Token length statistics (train):\")\n",
    "print(f\"  Min: {min(token_lengths)}\")\n",
    "print(f\"  Max: {max(token_lengths)}\")\n",
    "print(f\"  Mean: {sum(token_lengths)/len(token_lengths):.1f}\")\n",
    "print(f\"  Median: {sorted(token_lengths)[len(token_lengths)//2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tokens: 376425\n",
      "Unique tokens: 21090\n",
      "\n",
      "Top 20 most common words:\n",
      "  the: 20247\n",
      "  to: 10720\n",
      "  of: 8781\n",
      "  and: 8360\n",
      "  a: 7659\n",
      "  in: 6516\n",
      "  that: 5524\n",
      "  s: 4700\n",
      "  is: 4198\n",
      "  for: 3152\n",
      "  on: 2968\n",
      "  it: 2959\n",
      "  he: 2817\n",
      "  was: 2690\n",
      "  trump: 2688\n",
      "  this: 2350\n",
      "  with: 2249\n",
      "  as: 2222\n",
      "  i: 1923\n",
      "  his: 1904\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "for d in train_data:\n",
    "    all_tokens.extend(d['tokens'])\n",
    "word_freq = Counter(all_tokens)\n",
    "print(f\"\\nTotal tokens: {len(all_tokens)}\")\n",
    "print(f\"Unique tokens: {len(word_freq)}\")\n",
    "print(f\"\\nTop 20 most common words:\")\n",
    "for word, count in word_freq.most_common(20):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperlink Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperlink statistics (train):\n",
      "  Articles with links: 406\n",
      "  Total links: 2776\n",
      "  Mean per article: 4.3\n",
      "  Max: 277\n"
     ]
    }
   ],
   "source": [
    "hyperlink_counts = [len(d['hyperlinks']) for d in train_data]\n",
    "print(\"Hyperlink statistics (train):\")\n",
    "print(f\"  Articles with links: {sum(1 for c in hyperlink_counts if c > 0)}\")\n",
    "print(f\"  Total links: {sum(hyperlink_counts)}\")\n",
    "print(f\"  Mean per article: {sum(hyperlink_counts)/len(hyperlink_counts):.1f}\")\n",
    "print(f\"  Max: {max(hyperlink_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Link types: {'external': 2776}\n"
     ]
    }
   ],
   "source": [
    "link_types = Counter()\n",
    "for d in train_data:\n",
    "    for link in d['hyperlinks']:\n",
    "        link_types[link['type']] += 1\n",
    "print(f\"\\nLink types: {dict(link_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Hyperpartisan vs Non-Hyperpartisan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperpartisan articles: 238\n",
      "Non-hyperpartisan articles: 407\n",
      "\n",
      "Avg tokens (hyperpartisan): 820.4\n",
      "Avg tokens (non-hyperpartisan): 445.1\n",
      "\n",
      "Avg hyperlinks (hyperpartisan): 4.5\n",
      "Avg hyperlinks (non-hyperpartisan): 4.2\n"
     ]
    }
   ],
   "source": [
    "hyper = [d for d in train_data if d['label'] == 1]\n",
    "non_hyper = [d for d in train_data if d['label'] == 0]\n",
    "\n",
    "print(f\"Hyperpartisan articles: {len(hyper)}\")\n",
    "print(f\"Non-hyperpartisan articles: {len(non_hyper)}\")\n",
    "\n",
    "hyper_len = [len(d['tokens']) for d in hyper]\n",
    "non_hyper_len = [len(d['tokens']) for d in non_hyper]\n",
    "\n",
    "print(f\"\\nAvg tokens (hyperpartisan): {sum(hyper_len)/len(hyper_len):.1f}\")\n",
    "print(f\"Avg tokens (non-hyperpartisan): {sum(non_hyper_len)/len(non_hyper_len):.1f}\")\n",
    "\n",
    "hyper_links = [len(d['hyperlinks']) for d in hyper]\n",
    "non_hyper_links = [len(d['hyperlinks']) for d in non_hyper]\n",
    "\n",
    "print(f\"\\nAvg hyperlinks (hyperpartisan): {sum(hyper_links)/len(hyper_links):.1f}\")\n",
    "print(f\"Avg hyperlinks (non-hyperpartisan): {sum(non_hyper_links)/len(non_hyper_links):.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
